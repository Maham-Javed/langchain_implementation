from dotenv import load_dotenv
import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableParallel
from langchain_groq import ChatGroq
from langchain_core.output_parsers.string import StrOutputParser


load_dotenv()
api_key = os.getenv("GROQ_API_KEY")

# Use the updated model
model = ChatGroq(
    api_key=api_key,
    model="llama-3.1-8b-instant", 
    temperature=0.7 # temperature controls the randomness or creativity of the modelâ€™s responses
)

# Template for the first LLM call. It has two messages: a system and a human message with placeholders
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert {product_category} viewer."),
        ("human", "List me the features of {first_product} and {second_product}."),
    ]
)

# defining the global config
variables = {
    "product_category": "Car",
    "first_product": "Tesla",
    "second_product": "BYD",
}

# injecting the configs in the first chain so that we'll be able to use them in later stages without prop drilling
inject_config = RunnableLambda(lambda x: {**variables})

# extracting the pros or cons of the products from the content that is LLM generated by using first call.
def extract_products_arguments(products_features, consideration, product_category, product_name):
    # defining the prompt template for the pros case
    pros_template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are an expert {product_category} reviewer"),
            (
                "human",
                "Given these products features: {products_features}, give me 3 {consideration} of buying {product_name}.",
            ),
        ]
    )

    return pros_template.format_prompt(
        products_features = products_features,
        consideration = consideration,
        product_category = product_category,
        product_name = product_name,
    )

# returning each product pros and cons
def combine_pros_and_cons_of_product(product_name, pros, cons):
    return f"\n\n\nPros of {product_name}:\n\n\n {pros}\n\n\nCons of {product_name}\n\n\n {cons}"

# returning pros and cons for both products
def get_products_pros_and_cons(product_1_pros_and_cons, product_2_pros_and_cons):
    return f"\n\n\n\t\t\t-------------- Product 1 Pros & Cons are -------------- \n{product_1_pros_and_cons}\n\n\n\n\n\t\t\t-------------- Product 2 Pros & Cons are -------------- \n{product_2_pros_and_cons}\n\n"


# adding the Runnables for generating the prompt_template for product 1 pros and passing it to the LLM
product_1_pros_chain = (
        RunnableLambda(
            lambda x: extract_products_arguments(
                x, "pros", variables["product_category"], variables["first_product"]
                # here 'x' is the features response form LLM first call
            )
        )
    | model
    | StrOutputParser()
)

# adding the Runnables for generating the prompt_template for product 1 cons and passing it to the LLM
product_1_cons_chain = (
        RunnableLambda(
            lambda x: extract_products_arguments(
                x, "cons", variables["product_category"], variables["first_product"]
                # here 'x' is the features response form LLM first call
            )
        )
    | model
    | StrOutputParser()
)

# adding the Runnables for generating the prompt_template for product 2 pros and passing it to the LLM
product_2_pros_chain = (
        RunnableLambda(
            lambda x: extract_products_arguments(
                x, "pros", variables["product_category"], variables["second_product"]
                # here 'x' is the features response form LLM first call
            )
        )
    | model
    | StrOutputParser()
)

# adding the Runnables for generating the prompt_template for product 1 cons and passing it to the LLM
product_2_cons_chain = (
        RunnableLambda(
            lambda x: extract_products_arguments(
                x, "cons", variables["product_category"], variables["second_product"]
                # here 'x' is the features response form LLM first call
            )
        )
    | model
    | StrOutputParser()
)

pros_and_cons_for_product_1 = (
    RunnableLambda(
        lambda x: combine_pros_and_cons_of_product(
            variables["first_product"],
            x["branches"]["product_1_pros"],
            x["branches"]["product_1_cons"],
        )
    )
)

pros_and_cons_for_product_2 = (
    RunnableLambda(
        lambda x: combine_pros_and_cons_of_product(
            variables["second_product"],
            x["branches"]["product_2_pros"],
            x["branches"]["product_2_cons"],
        )
    )
)

# Create the combined chain using LangChain Expression Language (LCEL)
chains = (
    prompt_template
    | model
    | StrOutputParser()
    | RunnableParallel(
        branches = {
            "product_1_pros": product_1_pros_chain,
            "product_1_cons": product_1_cons_chain,
            "product_2_pros": product_2_pros_chain,
            "product_2_cons": product_2_cons_chain,
        }
    )
    | RunnableParallel(
        branches = {
            "pros_and_cons_for_product_1": pros_and_cons_for_product_1,
            "pros_and_cons_for_product_2": pros_and_cons_for_product_2        
            }
    )
    | RunnableLambda(
        lambda x: get_products_pros_and_cons(
            x["branches"]["pros_and_cons_for_product_1"],
            x["branches"]["pros_and_cons_for_product_2"],
        )
    )
)

result = chains.invoke(variables)

# Output
print(result)