{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Use the updated model\n",
    "model = ChatGroq(\n",
    "    api_key=api_key,\n",
    "    model=\"llama-3.1-8b-instant\", \n",
    "    temperature=0.7 # temperature controls the randomness or creativity of the model’s responses\n",
    ")\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "\n",
    "# write system message for giving the context to the LLM and store it in a chat history as a first message\n",
    "system_Message = SystemMessage(content=\"You are an helpfull AI assitence.\")\n",
    "chat_history.append(system_Message)\n",
    "\n",
    "while True:\n",
    "    # taking input form user\n",
    "    query = input(\"User: \")\n",
    "    if query.lower == \"exit\":\n",
    "        break\n",
    "\n",
    "    # adding the user question to the chat history\n",
    "    humanMessage = HumanMessage(content=query)\n",
    "    chat_history.append(humanMessage)\n",
    "\n",
    "    # providing the entire chat history to the model for better context\n",
    "    result = model.invoke(chat_history)\n",
    "\n",
    "    # Extracts the textual content from the returned model object\n",
    "    response = result.content\n",
    "\n",
    "    # Appends the assistant’s reply to chat_history so it will be included in future turns.\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    # Prints the assistant’s reply to the console.\n",
    "    print(f\"\\nAI: {response}\")\n",
    "\n",
    "print(\"\\n\\n----------Chat History----------\\n\\n\")\n",
    "# printing the entire chat history between the LLM and user\n",
    "print(f\"{chat_history}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
